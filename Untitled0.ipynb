{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWHfNGLOpMJd",
        "outputId": "512eea97-9ecf-46ee-a88d-d7a1334aa34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "bpKdbpO0oxOh",
        "outputId": "6b58fae6-b296-4218-f8e0-3acf18ddee35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15335 images belonging to 5 classes.\n",
            "Found 3830 images belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-59f33e866bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Use the fit_generator method to train the model using the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m history = search.fit(\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 )\n\u001b[1;32m     75\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0;34m\"Cannot clone object '%s' (type %s): \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;34m\"it does not seem to be a scikit-learn \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.functional.Functional object at 0x7f905239a820>' (type <class 'keras.engine.functional.Functional'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "\n",
        "# Load the ResNet50 model\n",
        "model = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "# Add a classification layer on top of the base model\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(5, activation='softmax')(x)\n",
        "model = Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model with an optimizer, loss function, and metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Load the data for image classification\n",
        "data_dir = '/content/drive/MyDrive/Dataset_2'\n",
        "\n",
        "# Use the ImageDataGenerator class to load the data from the directory and apply data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
        "                             horizontal_flip=True, vertical_flip=True,\n",
        "                             rotation_range=45, width_shift_range=0.2,\n",
        "                             height_shift_range=0.2, zoom_range=0.2)\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use the fit_generator method to train the model using the generators\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
        "               tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the training and validation accuracy and loss from the history object\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(acc, label='Training accuracy')\n",
        "plt.plot(val_acc, label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.title('Training and validation loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EKj_g5JN2_Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate predictions for the validation data\n",
        "predictions = model.predict(validation_generator)\n",
        "\n",
        "# Get the class labels from the generator\n",
        "labels = validation_generator.classes\n",
        "\n",
        "# Convert the predictions and labels to numpy arrays\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_matrix = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tKOcFK-c2n2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = validation_generator.classes\n",
        "y_pred = model.predict_generator(validation_generator)\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Plot the precision and recall for each class\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "disp = plot_precision_recall_curve(model, validation_generator, y_true)\n",
        "disp.ax_.set_title('Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(disp.average_precision))"
      ],
      "metadata": {
        "id": "JTUgF3UGpoIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVhI4wb239xy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}